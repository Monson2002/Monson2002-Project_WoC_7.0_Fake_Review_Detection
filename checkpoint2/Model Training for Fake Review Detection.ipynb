{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "      <th>cleaned_reviews</th>\n",
       "      <th>tokenised_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28218</th>\n",
       "      <td>Kindle_Store_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>['recieved', 'arc', 'copy', 'book', 'exchange'...</td>\n",
       "      <td>recieved arc copy book exchange honest reviewt...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36343</th>\n",
       "      <td>Toys_and_Games_5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>['son', 'toy', 'material', 'good', 'love', 'fi...</td>\n",
       "      <td>son toy material good love figure color bright...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10113</th>\n",
       "      <td>Electronics_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>['say', 'thisit', 'basically', 'new', 'came', ...</td>\n",
       "      <td>say thisit basically new came orginal box conn...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>Sports_and_Outdoors_5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>['sugary', 'tasty', 'similar', 'ice', 'cream',...</td>\n",
       "      <td>sugary tasty similar ice cream cone also love ...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31371</th>\n",
       "      <td>Books_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>['loved', 'book', 'usual', 'character', 'well'...</td>\n",
       "      <td>loved book usual character well developed beli...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    category  rating label  \\\n",
       "28218         Kindle_Store_5     5.0    CG   \n",
       "36343       Toys_and_Games_5     4.0    CG   \n",
       "10113          Electronics_5     5.0    OR   \n",
       "4968   Sports_and_Outdoors_5     4.0    CG   \n",
       "31371                Books_5     5.0    CG   \n",
       "\n",
       "                                                   text_  \\\n",
       "28218  ['recieved', 'arc', 'copy', 'book', 'exchange'...   \n",
       "36343  ['son', 'toy', 'material', 'good', 'love', 'fi...   \n",
       "10113  ['say', 'thisit', 'basically', 'new', 'came', ...   \n",
       "4968   ['sugary', 'tasty', 'similar', 'ice', 'cream',...   \n",
       "31371  ['loved', 'book', 'usual', 'character', 'well'...   \n",
       "\n",
       "                                         cleaned_reviews  \\\n",
       "28218  recieved arc copy book exchange honest reviewt...   \n",
       "36343  son toy material good love figure color bright...   \n",
       "10113  say thisit basically new came orginal box conn...   \n",
       "4968   sugary tasty similar ice cream cone also love ...   \n",
       "31371  loved book usual character well developed beli...   \n",
       "\n",
       "             tokenised_reviews  \n",
       "28218  [0. 0. 0. ... 0. 0. 0.]  \n",
       "36343  [0. 0. 0. ... 0. 0. 0.]  \n",
       "10113  [0. 0. 0. ... 0. 0. 0.]  \n",
       "4968   [0. 0. 0. ... 0. 0. 0.]  \n",
       "31371  [0. 0. 0. ... 0. 0. 0.]  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./preprocessed_dataset.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processor(review):\n",
    "  nopunc = [char for char in review if char not in string.punctuation]\n",
    "  nopunc = ''.join(nopunc)\n",
    "  return [word for word in nopunc.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary: 43454\n"
     ]
    }
   ],
   "source": [
    "bow = CountVectorizer(analyzer=text_processor)\n",
    "bow.fit(df['text_'])\n",
    "print(\"Total Vocabulary:\",len(bow.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15838    ['easy', 'install', 'easy', 'hook', 'bike', 'g...\n",
       " 7673     ['purchased', 'one', 'navy', 'speedo', 'outdon...\n",
       " 22911    ['great', 'idea', 'design', 'isnt', 'good', 'e...\n",
       " 32135    ['first', 'title', 'book', 'misleading', 'titl...\n",
       " 11064    ['slim', 'mouse', 'sleek', 'feel', 'comfortabl...\n",
       " Name: text_, dtype: object,\n",
       " 23176    OR\n",
       " 33766    CG\n",
       " 6856     OR\n",
       " 22249    OR\n",
       " 26202    OR\n",
       " Name: label, dtype: object)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['text_']\n",
    "y = df['label']\n",
    "X.sample(5), y.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline to train models using Random Forest, SVM, and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "  'randomForest': Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "  ]),\n",
    "  'svc': Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', SVC(random_state=42))\n",
    "  ]),\n",
    "  'logistic': Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(random_state=42))\n",
    "  ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting and Saving them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a randomForest classifier\n",
      "Training a svc classifier\n",
      "Training a logistic classifier\n"
     ]
    }
   ],
   "source": [
    "for model, pipeline in pipelines.items():\n",
    "  print(f'Training a {model} classifier')\n",
    "  pipeline.fit(X_train, y_train)\n",
    "  joblib.dump(pipeline, f'{model}_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation using Accuracy, Precision, Recall, and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "  'accuracy': accuracy_score,\n",
    "  'precision': lambda y_true, y_pred: precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "  'recall': lambda y_true, y_pred: recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "  'f1': lambda y_true, y_pred: f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model, pipeline in pipelines.items():\n",
    "  y_pred = pipeline.predict(X_test)\n",
    "  y_test_np = y_test.to_numpy()\n",
    "  model_metrics = {}  \n",
    "  for metric_name, value in metrics.items():\n",
    "    model_metrics[metric_name] = value(y_test_np, y_pred)\n",
    "  \n",
    "  results[model] = model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Accuracy, Precision, Recall, and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest\n",
      "accuracy  0.8522\n",
      "precision  0.8549\n",
      "recall  0.8522\n",
      "f1  0.8519\n",
      "\n",
      "Svc\n",
      "accuracy  0.8926\n",
      "precision  0.8930\n",
      "recall  0.8926\n",
      "f1  0.8926\n",
      "\n",
      "Logistic\n",
      "accuracy  0.8711\n",
      "precision  0.8714\n",
      "recall  0.8711\n",
      "f1  0.8711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model, metrics in results.items():\n",
    "  print(f'{model[0].upper() + model[1:]}')\n",
    "  for metric_name, value in metrics.items():\n",
    "    print(f'{metric_name} {value: .4f}')\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomForest': 0.8527816857969919,\n",
       " 'svc': 0.892723299096074,\n",
       " 'logistic': 0.8711802105152182}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance_measure = {}\n",
    "\n",
    "for model, metrics in results.items():\n",
    "  metricSum = 0\n",
    "  for metric_name, value in metrics.items():\n",
    "    metricSum += value\n",
    "  metricSum /= 4\n",
    "  model_performance_measure[model] = metricSum\n",
    "\n",
    "model_performance_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since SVC has the highest weighted performance metric sum, it can be said as the best model we can use to fit our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomForest predictions: ['OR' 'OR']\n",
      "svc predictions: ['CG' 'OR']\n",
      "logistic predictions: ['CG' 'OR']\n"
     ]
    }
   ],
   "source": [
    "sample_data = [\n",
    "  \"This product is great, I loved it!\",\n",
    "  \"Terrible experience, wouldn't recommend it.\"\n",
    "]\n",
    "\n",
    "for model_name in pipelines:\n",
    "  model = joblib.load(f\"{model_name}_model.pkl\")\n",
    "  predictions = model.predict(sample_data)\n",
    "  print(f\"{model_name} predictions: {predictions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
